---
layout: post
title: Finding service outages with robust statistics
date: 2021-02-12T12:30:00-08:00
category: Engineering
tags:
  - monitoring
  - outliers
  - statistics
---

<script>
    import {onMount} from "svelte";
    import Table from "../../components/Table.svelte";
    import Plot from  "../../components/Plot.svelte";
    import Visualization from "./_assets/2021-02-12-viz.svelte"

    import {
        mean,
        median,
        standardDeviation,
        medianAbsoluteDeviation
    } from "simple-statistics";

    let rawData = []
    let data = []
    $: delta = data.map(row => row.delta);
    let dataDec = []
    let dataOct = []

    const columns = [
        {
            name: "last update",
            format: (row) => row.last_update.slice(0, 16)
        },
        {
            name: "delta (hour)",
            format: (row) => row.delta
        }
    ];

    function transform(data) {
        return [
            {
                x: data.map(row => row.last_update),
                y: data.map(row => row.delta),
                type: "line",
                name: "delta"
            }
        ]
    }

    function withScores(data) {
        // first the standard deviation
        let deltas = data.map(row => row.delta)
        let std = standardDeviation(deltas);
        let mad = medianAbsoluteDeviation(deltas)
        let k = 1.4826
        return data.map(row => ({
            ...row,
            scoreStd: (row.delta - mean(deltas))/std,
            scoreMad: (row.delta - median(deltas))/(k*mad)
        }));
    }

    onMount(async () => {
        let resp = await fetch("assets/2021-02-12/probeinfo_status.json");
        rawData = await resp.json();
        data = withScores(rawData);
        dataDec = withScores(rawData.filter(row => row.last_update.slice(5,7) == "11"))
        dataOct = withScores(rawData.filter(row => row.last_update.slice(5,7) == "10"))
    })
</script>

_This post is interactive and may not render correctly without Javascript._

The [Probe Scraper][probe-scraper] underpins the data schema infrastructure at
Mozilla. Every night, it trawls through the Firefox Mercurial and Git
repositories searching for registry files. The output powers an [Probe-Info
Service API][probe-info] (probe-info) that is used to [generate
schemas][schema-generator] and power [data dictionaries][glean-dictionary].

It is scheduled to run on business days (Monday to Friday UTC+00) on
[Airflow][airflow]. While there are notifications to the data engineering team
when the job inevitably fails for one reason or another, the status of the
[schema deployment pipeline][docs] has not always been clear from the outside.

In this post, I'll write a little bit about how the probe-info service is
monitored and and displayed for operational transparency. I'll focus the
discussion on the merits of statistical tools like the median absolute deviation
(MAD) for figuring out whether the probe-info service is up to date without
actually having access to the internals of the service.

[probe-scraper]: https://github.com/mozilla/probe-scraper
[probe-info]: https://mozilla.github.io/probe-scraper/
[schema-generator]: https://github.com/mozilla/mozilla-schema-generator
[glean-dictionary]: https://github.com/mozilla/glean-dictionary
[airflow]: https://github.com/mozilla/telemetry-airflow/
[docs]: https://docs.telemetry.mozilla.org/concepts/pipeline/schemas.html

{#if data.length > 0}

## Collecting monitoring data

Every 15 minutes, and endpoint from the probe-info service is queried to obtain
the last update timestamp. This gets loaded into a BigQuery table, then
transformed and dumped into a JSON file. During this process, the series of
timestamps is transformed into deltas representing the time since the last
update. In the SQL, we encode some of the business logic to take into account
the lull of the weekends.

We expect to see five updates a week at a regular interval of 24 hours. We can
plot this data to see obvious irregularities with the date. We consider anything
that takes longer than 24 business hours a partial-outage. It is partial because
the infrastructure continues to serve requests despite out of date information
about schemas.

<div style="border: 1px solid black; padding: 1em;">

<Plot
{data}
transform={transform}
layout={{
    title:"Time since last deploy (hours)",
    height: 320,
    }}
/>

<br>

<details>
<summary>Click to reveal tabular data</summary>
<Table {data} columns={columns} paginationSize={7} />
</details>

<br>

<details>
<summary>Click to reveal the first 3 rows of the raw JSON</summary>
<pre>{JSON.stringify(rawData.slice(0, 3), '', 2)}</pre>
</details>

</div>

You can obtain a copy of the frozen output [here][status] or updated data
directly from the [monitoring dashboard][monitoring].

The large spike on 2021-01-28 was due to a memory pressure issue that took
several days to resolve. This is the largest partial-outage in recorded history,
but not the only one. Between November and December, there are five outages in
total.

Is there a way to automatically detect whether a particular day is experiencing
a partial-outage, without knowing the intricacies of scheduling? It turns out we
can do this inference easily with the help of statistics.

[status]: assets/2021-02-12/probeinfo_status.json
[monitoring]: https://protosaur.dev/mps-deploys/

## A refresher on statistics, are you MADâ€½

The mean is {mean(delta).toFixed(1)} ($\mu$) with a standard deviation
($$\sigma$$) of {standardDeviation(delta).toFixed(1)}. We can determine how far
a point is from the mean by computing a z-score.

$$
z = \frac{x - \mu}{\sigma}
$$

The median ($\tilde{\mu}$) is {median(delta)} with a median absolute deviation
($MAD$) of {medianAbsoluteDeviation(delta)}. We can modify the z-score to give
us a robust equivalent to the standard deviation.

$$
\tilde{z} = \frac{x - \tilde{\mu}}{k \cdot MAD}
$$

## Detecting partial-outages on historical data

Now armed with the necessary statistical tools, we'll run this on some
historical data. A partial-outage is a period of time where the probe-scraper
service has not successfully run. The outage is partial because our schema
repository continues to work outside of this subsystem. However, it may not have
the most up to date information about the set of metrics.

### All of history (7 partial-outages)

First lets take a look at the data over the entire dataset so far.

<details>
<summary>Click to reveal analysis</summary>
<div style="border: 1px solid black; padding: 1em;">
<Visualization data={data} />
</div>
</details>

### Month of November 2020 (3 partial-outages)

Now lets repeat it for the month of November. This is a period with three
partial outages.

<details>
<summary>Click to reveal analysis</summary>
<div style="border: 1px solid black; padding: 1em;">
<Visualization data={dataDec} />
</div>
</details>

### Month of October 2020 (no outages)

And again for the month of October. This was a perfect month without any outages ðŸ˜Š.

<details>
<summary>Click to reveal analysis</summary>
<div style="border: 1px solid black; padding: 1em;">
<Visualization data={dataOct} />
</div>
</details>

{/if}

## Thoughts and closing remarks

MAD works pretty well in all sorts of situations because it is robust.

## References

- https://www.statisticshowto.com/probability-and-statistics/z-score/
- https://en.wikipedia.org/wiki/Median_absolute_deviation
- http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/BetterThanMAD.pdf
- https://www.itl.nist.gov/div898/handbook/eda/section3/eda35h.htm
- https://docs.oracle.com/cd/E17236_01/epm.1112/cb_statistical/frameset.htm?ch07s02s10s01.html
